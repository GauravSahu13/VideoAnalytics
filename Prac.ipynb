{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N1SaH-K2WbR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5xn9Fb43AE8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Travel.csv\")\n",
        "df.head()\n",
        "df.info()\n",
        "df.describe()\n",
        "df.shape\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACVUigbr3VGY"
      },
      "outputs": [],
      "source": [
        "missing = df.isnull().mean()\n",
        "missing.sort_values(ascending=False)*100\n",
        "\n",
        "df['Embarked'].fillna(df.Embarked.mode()[0], inplace=True)\n",
        "\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "#df['Age'] = df['Age'].replace(0,df['Age'].mean())\n",
        "\n",
        "\n",
        "df.drop('col',axis=1,inplace=True)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejdxlu6L3ep6"
      },
      "outputs": [],
      "source": [
        "# AutoEDA\n",
        "!pip install pydantic-settings\n",
        "from ydata_profiling import ProfileReport as pp\n",
        "profile = pp(df, title=\"Data Profile Report\", explorative=True)\n",
        "profile.to_file(\"data_profile_final_raw.html\")\n",
        "print(\"The profiling report has been generated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mAqSYnV3hKv"
      },
      "outputs": [],
      "source": [
        "#Outlier\n",
        "sns.boxplot(df)\n",
        "def handle_outliers_iqr(df):\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            q1 = df[col].quantile(0.25)\n",
        "            q3 = df[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 - 1.5 * iqr\n",
        "            upper_bound = q3 + 1.5 * iqr\n",
        "            df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
        "    return df\n",
        "df2 = handle_outliers_iqr(df)\n",
        "sns.boxplot(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElurJiCU3qrW"
      },
      "outputs": [],
      "source": [
        "# prompt: eda for datatime feature in dataset\n",
        "\n",
        "# Convert 'date_time' column to datetime objects\n",
        "df['date_time'] = pd.to_datetime(df['date_time'])\n",
        "\n",
        "# Extract features from the datetime column\n",
        "df['year'] = df['date_time'].dt.year\n",
        "df['month'] = df['date_time'].dt.month\n",
        "df['day'] = df['date_time'].dt.day\n",
        "\n",
        "# Analyze the extracted features\n",
        "# Example: Plot the traffic volume by hour of day\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='hour', y='traffic_volume', data=df)\n",
        "plt.title('Traffic Volume by Hour of Day')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP6kjUoD4G1o"
      },
      "outputs": [],
      "source": [
        "# CORRELATION MATRIX\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(df.corr(),annot=True)\n",
        "plt.title(\"Correlation between the columns\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4LRnrt430vI"
      },
      "outputs": [],
      "source": [
        "# splitting into num and categorical\n",
        "num_cols = df.select_dtypes(include=np.number).columns\n",
        "cat_cols = df.select_dtypes(exclude=np.number).columns\n",
        "num_cols, cat_cols\n",
        "\n",
        "# pie chart for categorical\n",
        "\n",
        "for col in cat_cols:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.pie(df[col].value_counts().values, labels=df[col].value_counts().index, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title(col)\n",
        "    plt.show()\n",
        "\n",
        "for col in num_cols:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(col)\n",
        "    plt.xlim(0, 450)\n",
        "    plt.show()\n",
        "\n",
        "# aur directly use pairplot\n",
        "\n",
        "sns.pairplot(df)\n",
        "\n",
        "sns.regplot(x = 'Age', y = 'Fare', data = df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2G_vntA4OWe"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder,OrdinalEncoder,LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['weather_main'] = le.fit_transform(df['weather_main'])\n",
        "df['weather_description']= le.fit_transform(df['weather_description'])\n",
        "\n",
        "# Perform one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['weather_main', 'weather_description'], prefix=['weather_main', 'weather_description'],drop_first = True)\n",
        "print(df.head())\n",
        "\n",
        "dict = {'S' : 1, 'C': 2, 'Q' : 3}\n",
        "df['Embarked'] = df['Embarked'].map(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7LpDtv26f8W"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "\n",
        "X = df.drop(['traffic_volume','date_time'], axis=1)\n",
        "y = df['traffic_volume']\n",
        "X.shape,y.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmFl6cWY6tiU"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "models_and_params = {\n",
        "    'LinearRegression': {\n",
        "        'model': LinearRegression(),\n",
        "        'params': {\n",
        "            'fit_intercept': [True, False],\n",
        "        }\n",
        "    },\n",
        "    'RandomForestRegressor': {\n",
        "        'model': RandomForestRegressor(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 150, 200, 300],\n",
        "            'max_depth': [None, 5, 10, 15, 20],\n",
        "        }\n",
        "    },\n",
        "    'DecisionTreeRegressor': {\n",
        "        'model': DecisionTreeRegressor(),\n",
        "        'params': {\n",
        "            'max_depth': [None, 5, 10, 15, 20],\n",
        "            'min_samples_split': [2, 5, 10, 15],\n",
        "        }\n",
        "    },\n",
        "    'XGBRegressor': {\n",
        "        'model': XGBRegressor(eval_metric='rmse'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 150, 200],\n",
        "            'max_depth': [3, 5, 7, 9],\n",
        "            'learning_rate': [0.01, 0.1, 0.2, 0.3]\n",
        "        }\n",
        "    },\n",
        "    'AdaBoostRegressor': {\n",
        "        'model': AdaBoostRegressor(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'learning_rate': [0.1, 0.5, 1, 2]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model_info in models_and_params.items():\n",
        "    model = model_info['model']\n",
        "    params = model_info['params']\n",
        "    grid = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_model = grid.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = mse ** 0.5\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'best_params': grid.best_params_,\n",
        "        'best_score': grid.best_score_,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "    })\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.round({'best_score': 2, 'mse': 2, 'rmse': 2, 'r2': 2})\n",
        "print(results_df[['model', 'best_params', 'best_score', 'mse', 'rmse', 'r2']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7vrwMZJ2_Im"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay,precision_score, recall_score, f1_score, roc_auc_score,roc_curve\n",
        "\n",
        "\n",
        "models_and_params = {\n",
        "    'RandomForestClassifier': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 150, 200, 300],\n",
        "            'max_depth': [None, 5, 10, 15, 20],\n",
        "        }\n",
        "    },\n",
        "    'DecisionTreeClassifier': {\n",
        "        'model': DecisionTreeClassifier(),\n",
        "        'params': {\n",
        "            'criterion': ['gini', 'entropy']\n",
        "            'max_depth': [None, 5, 10, 15, 20],\n",
        "            'min_samples_split': [2, 5, 10, 15],\n",
        "        }\n",
        "    },\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(solver='liblinear'),\n",
        "        'params': {\n",
        "            'penalty': ['l1', 'l2']\n",
        "            'C': [0.01, 0.1, 1, 10, 100],\n",
        "            'max_iter': [100, 200, 300, 500]\n",
        "        }\n",
        "    },\n",
        "    'XGBClassifier': {\n",
        "        'model': XGBClassifier(eval_metric='mlogloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 150, 200],\n",
        "            'max_depth': [3, 5, 7, 9],\n",
        "            'learning_rate': [0.01, 0.1, 0.2, 0.3]\n",
        "        }\n",
        "    },\n",
        "    'AdaBoostClassifier': {\n",
        "        'model': AdaBoostClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'learning_rate': [0.1, 0.5, 1, 2]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model_info in models_and_params.items():\n",
        "    model = model_info['model']\n",
        "    params = model_info['params']\n",
        "    grid = GridSearchCV(model, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)  # Train on the training set\n",
        "    best_model = grid.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'best_params': grid.best_params_,\n",
        "        'best_score': grid.best_score_,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    })\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.round({'best_score': 2, 'accuracy': 2, 'precision': 2, 'recall': 2, 'f1': 2})\n",
        "print(results_df[['model', 'best_params', 'best_score', 'accuracy', 'precision', 'recall', 'f1']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbbXy1zj7lOC"
      },
      "outputs": [],
      "source": [
        "Rf_model=RandomForestClassifier()\n",
        "Rf_model.fit(X_train,y_train)\n",
        "y_pred_rf=Rf_model.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred_rf))\n",
        "# we are tuning three hyperparameters right now, we are passing the different values for both parameters\n",
        "'''grid_param = {\n",
        "    \"n_estimators\" : [90,100,115,130],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth' : range(2,20,1),\n",
        "    'min_samples_leaf' : range(1,10,1),\n",
        "    'min_samples_split': range(2,10,1),\n",
        "    'max_features' : ['auto','log2']\n",
        "}\n",
        "\n",
        "grid_searh=GridSearchCV(estimator=Rf_model,param_grid=grid_param,cv=3,verbose=2,n_jobs=-1)\n",
        "grid_searh.fit(X_train,y_train)\n",
        "print(grid_searh.best_params_)\n",
        "y_pred=grid_searh.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3OECsdm8d1-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/train.mnist.csv')\n",
        "X = df.iloc[:, 1:]\n",
        "y = df.iloc[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create an imputer to fill NaN values with the mean of each column\n",
        "imputer = SimpleImputer(strategy='mean') # You can use other strategies like 'median' or 'most_frequent'\n",
        "\n",
        "# Fit the imputer to your training data and transform it\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted imputer\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Now, you can proceed with training your model\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZpbYPX_8fce"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "import plotly.express as px\n",
        "y_train_scaled = y_train.astype(str)\n",
        "fig = px.scatter_3d(df, x=X_train_pca[:,0],\n",
        "                    y=X_train_pca[:,1],\n",
        "                    z=X_train_pca[:,2],\n",
        "                    color=y_train_scaled)\n",
        "fig.update_layout(margin= dict(l=20, r=20, b=20, t=20),\n",
        "                  paper_bgcolor='LightSteelBlue')\n",
        "fig.show()\n",
        "\n",
        "pca.explained_variance_    #variance of the components PCA1, PCA2, PCA3\n",
        "pca.explained_variance_ratio_    # atleast 80 % required\n",
        "np.cumsum(pca.explained_variance_ratio_)          # cumulative sum\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezYi-MeV8wSB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "label = LabelEncoder()\n",
        "y = label.fit_transform(y)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters = 2, random_state = 42)\n",
        "kmeans.fit(X)\n",
        "kmeans.cluster_centers_\n",
        "kmeans.labels_\n",
        "kmeans.inertia_\n",
        "silhouette_score(X, kmeans.labels_)\n",
        "\n",
        "\n",
        "inertia =[]\n",
        "for k in range(1,11):\n",
        "  kmeans = KMeans(n_clusters = k, random_state = 42)\n",
        "  kmeans.fit(X)\n",
        "  inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1,11),inertia,marker='o', linestyle='--')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters = 3, random_state = 42)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "correct_labels = sum(y== labels)\n",
        "total_labels = len(labels)\n",
        "accuracy = correct_labels/total_labels\n",
        "print(accuracy)\n",
        "\n",
        "print(\"Result %d out of %d samples corectly labelled\" % (correct_labels, y.size))\n",
        "\n",
        "print('Accuracy score: ', format(correct_labels/float(y.size)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
